#!/usr/bin/env python3
"""
BALIZERO INTEL PIPELINE - Complete Flow Orchestrator
=====================================================
The FULL pipeline from RSS to published article with image.

Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  0. SEMANTIC DEDUPLICATION (Qdrant)                             â”‚
â”‚     - Check vector similarity > 88% with recent news            â”‚
â”‚     - Skip immediately if duplicate (Save $$$)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. RSS FETCHER                                                 â”‚
â”‚     Raw article: {title, summary, url}                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. LLAMA SCORER (fast, local, free)                            â”‚
â”‚     - Keyword matching + heuristics                             â”‚
â”‚     - Score 0-100, category, priority                           â”‚
â”‚     - Filters obvious noise (score < 40)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. CLAUDE VALIDATOR (intelligent gate)                         â”‚
â”‚     - For ambiguous scores (40-75)                              â”‚
â”‚     - Quick research/validation                                 â”‚
â”‚     - Decides: "Worth enriching?"                               â”‚
â”‚     - Can override LLAMA's category/priority                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“ (only approved)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. CLAUDE MAX ENRICHMENT                                       â”‚
â”‚     - Fetches full article content                              â”‚
â”‚     - Writes complete Executive Brief                           â”‚
â”‚     - BaliZero style, actionable insights                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. CLAUDE IMAGE REASONING                                      â”‚
â”‚     - Reads the enriched article                                â”‚
â”‚     - Reasons: What scene captures this?                        â”‚
â”‚     - Creates unique Gemini prompt                              â”‚
â”‚     - Browser automation generates image                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5.5 SEO/AEO OPTIMIZATION                                       â”‚
â”‚     - Schema.org JSON-LD structured data                        â”‚
â”‚     - Meta tags (OG, Twitter, canonical)                        â”‚
â”‚     - TL;DR summary for AI citation                             â”‚
â”‚     - FAQ generation for featured snippets                      â”‚
â”‚     - Entity extraction for LLM knowledge                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. SUBMIT FOR APPROVAL (parallel)                              â”‚
â”‚     6a. News Room UI â†’ zantara.balizero.com/intelligence        â”‚
â”‚         (Frontend deployed on Vercel, custom domain)            â”‚
â”‚     6b. Telegram â†’ voting via bot (2/3 majority)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. AUTO-MEMORY (Qdrant)                                        â”‚
â”‚     - Save article vector for future deduplication              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Cost breakdown:
- Qdrant check: $0 (included in infrastructure)
- LLAMA scoring: $0 (local Ollama)
- Claude validation: ~$0.01 per article (quick validation)
- Claude Max enrichment: ~$0.05 per article (full article)
- Gemini image: $0 (Google One AI Premium)
- SEO/AEO optimization: $0 (local processing)
"""

import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field, asdict
import httpx

# Import pipeline components
from professional_scorer import score_article
from ollama_scorer import OllamaScorer
from claude_validator import ClaudeValidator
from article_deep_enricher import ArticleDeepEnricher, EnrichedArticle
from gemini_image_generator import GeminiImageGenerator
from seo_aeo_optimizer import SEOAEOOptimizer, optimize_article as seo_optimize
from telegram_approval import TelegramApproval
from metrics import MetricsCollector, track_latency, StructuredLogger
from logging_config import setup_logging, get_logger, log_context, correlation_context, PerformanceLogger
# Usa versione httpx per evitare problemi TLS con qdrant-client
try:
    from semantic_deduplicator_httpx import SemanticDeduplicator
except ImportError:
    # Fallback alla versione originale se httpx non disponibile
    from semantic_deduplicator import SemanticDeduplicator

# Configure logging with centralized config
setup_logging(app_name="intel_pipeline", log_dir="logs")
logger = get_logger("intel_pipeline")


@dataclass
class PipelineArticle:
    """Article as it flows through the pipeline"""

    # Original data
    title: str
    summary: str
    url: str
    source: str
    content: str = ""
    published_at: Optional[str] = None

    # Deduplication
    is_duplicate: bool = False
    duplicate_of: str = ""
    similarity_score: float = 0.0

    # LLAMA scoring results
    llama_score: int = 0
    llama_category: str = "general"
    llama_priority: str = "medium"
    llama_reason: str = ""
    llama_keywords: List[str] = field(default_factory=list)

    # Claude validation results
    validated: bool = False
    validation_approved: bool = False
    validation_confidence: int = 0
    validation_reason: str = ""
    category_override: Optional[str] = None
    priority_override: Optional[str] = None
    enrichment_hints: List[str] = field(default_factory=list)

    # Enrichment results
    enriched: bool = False
    enriched_article: Optional[EnrichedArticle] = None

    # Image generation
    image_prompt: str = ""
    image_path: str = ""

    # SEO/AEO optimization
    seo_optimized: bool = False
    seo_metadata: Dict = field(default_factory=dict)

    # Approval workflow
    pending_approval: bool = False
    approval_id: str = ""
    approval_status: str = ""  # pending, approved, rejected
    preview_url: str = ""  # URL to dark-theme preview page

    # E-E-A-T Human Review tracking
    requires_human_review: bool = True  # Default: all articles require human review
    reviewed_by: str = ""  # Name/email of reviewer (for E-E-A-T compliance)
    reviewed_at: Optional[str] = None  # ISO timestamp of review

    # Final status
    published: bool = False
    publish_result: Dict = field(default_factory=dict)

    @property
    def final_category(self) -> str:
        """Get final category (with override if present)"""
        return self.category_override or self.llama_category

    @property
    def final_priority(self) -> str:
        """Get final priority (with override if present)"""
        return self.priority_override or self.llama_priority


@dataclass
class PipelineStats:
    """Pipeline execution statistics"""

    total_input: int = 0
    dedup_filtered: int = 0
    llama_scored: int = 0
    llama_filtered: int = 0
    claude_validated: int = 0
    claude_approved: int = 0
    claude_rejected: int = 0
    enriched: int = 0
    images_generated: int = 0
    seo_optimized: int = 0
    pending_approval: int = 0
    pending_human_review: int = 0  # E-E-A-T: articles awaiting human review
    published: int = 0
    errors: int = 0
    duration_seconds: float = 0


class IntelPipeline:
    """
    Complete intelligence pipeline orchestrator.

    Coordinates all components:
    - Qdrant for semantic deduplication
    - LLAMA for fast scoring
    - Claude for validation
    - Claude Max for enrichment
    - Gemini for images
    """

    def __init__(
        self,
        min_llama_score: int = 40,
        auto_approve_threshold: int = 75,
        generate_images: bool = True,  # Always True - images are mandatory
        require_approval: bool = True,
        require_human_review: bool = True,  # E-E-A-T: human must review before publish
        dry_run: bool = False,
    ):
        self.min_llama_score = min_llama_score
        self.auto_approve_threshold = auto_approve_threshold
        self.generate_images = generate_images
        self.require_approval = require_approval
        self.require_human_review = require_human_review  # E-E-A-T compliance
        self.dry_run = dry_run

        # Initialize components
        self.deduplicator = SemanticDeduplicator()
        self.ollama_scorer = OllamaScorer()
        self.claude_validator = ClaudeValidator()
        # Images are mandatory - always enable generation
        self.generate_images = True
        self.enricher = ArticleDeepEnricher(generate_images=True)
        # Initialize image generator (mandatory)
        try:
            self.image_generator = GeminiImageGenerator()
            logger.info("âœ… Gemini Image Generator initialized (mandatory)")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize Gemini Image Generator (mandatory): {e}")
            raise RuntimeError(f"Image generator is mandatory but failed to initialize: {e}")
        self.seo_optimizer = SEOAEOOptimizer()
        self.approval_system = TelegramApproval() if require_approval else None

        # Stats (legacy)
        self.stats = PipelineStats()

        # Enhanced Metrics (Prometheus-compatible)
        self.metrics = MetricsCollector(app_name="bali_intel_pipeline")
        self.metrics_logger = StructuredLogger("pipeline", metrics=self.metrics)

        # Verify critical dependencies
        self._verify_dependencies()

        logger.info("=" * 70)
        logger.info("ğŸš€ BALIZERO INTEL PIPELINE INITIALIZED (v6.6 - E-E-A-T Ready)")
        logger.info(f"   Min LLAMA score: {min_llama_score}")
        logger.info(f"   Auto-approve threshold: {auto_approve_threshold}")
        logger.info(f"   Generate images: {generate_images}")
        logger.info(f"   Require approval: {require_approval}")
        logger.info(f"   Require human review: {require_human_review} (E-E-A-T)")
        logger.info(f"   Dry run: {dry_run}")
        logger.info("=" * 70)
    
    def _verify_dependencies(self):
        """Verify critical dependencies are available"""
        import os
        
        # Check OpenAI API key (required for semantic deduplication)
        if not os.getenv("OPENAI_API_KEY"):
            logger.warning("âš ï¸ OPENAI_API_KEY not set - semantic deduplication will fail")
        
        # Check Qdrant credentials
        if not os.getenv("QDRANT_API_KEY"):
            logger.warning("âš ï¸ QDRANT_API_KEY not set - Qdrant operations will fail")
        
        # Check backend URL
        backend_url = os.getenv("BACKEND_API_URL", "https://nuzantara-rag.fly.dev")
        logger.debug(f"Backend URL: {backend_url}")

    async def send_to_news_room(self, article: PipelineArticle, preview_url: str = None) -> bool:
        """
        Send article to backend Intelligence Center News Room.

        This populates https://zantara.balizero.com/intelligence/news-room
        (frontend deployed on Vercel, custom domain) with articles for team
        review in parallel with Telegram notifications.

        Args:
            article: PipelineArticle to send
            preview_url: Optional URL to the dark-theme preview page (for E-E-A-T review)
        """
        backend_url = os.getenv("BACKEND_API_URL", "https://nuzantara-rag.fly.dev")
        endpoint = f"{backend_url}/api/intel/scraper/submit"

        # Get enriched content if available
        enriched = article.enriched_article
        # EnrichedArticle doesn't have executive_brief, use ai_summary or facts
        if enriched:
            # Build full enriched content
            content_parts = []
            if enriched.ai_summary:
                content_parts.append(f"## Summary\n{enriched.ai_summary}")
            if enriched.facts:
                content_parts.append(f"## Facts\n{enriched.facts}")
            if enriched.bali_zero_take:
                content_parts.append(f"## Bali Zero Take\n{enriched.bali_zero_take}")
            if enriched.next_steps:
                next_steps_str = "\n".join([f"- {k}: {v}" for k, v in enriched.next_steps.items()])
                content_parts.append(f"## Next Steps\n{next_steps_str}")

            content = "\n\n".join(content_parts) if content_parts else article.summary
            title = enriched.headline or article.title
        else:
            content = article.summary
            title = article.title

        payload = {
            "title": title,
            "content": content,
            "source_url": article.url,
            "source_name": article.source,
            "category": article.final_category,
            "relevance_score": article.llama_score,
            "published_at": article.published_at,
            "extraction_method": "intel_pipeline",
            "tier": "T2",  # Default tier for pipeline articles
        }

        # Add preview URL for E-E-A-T human review (balizero.com dark-theme preview)
        if preview_url:
            payload["preview_url"] = preview_url
            logger.info(f"   ğŸ–¼ï¸ Preview URL included: {preview_url}")

        # Add SEO metadata if available
        if article.seo_metadata:
            payload["seo_metadata"] = article.seo_metadata
            logger.info("   ğŸ” SEO metadata included")

        # Cover image is mandatory for enriched articles
        if enriched:
            if not enriched.cover_image:
                logger.error(f"   âŒ Cover image missing (mandatory) for article: {title[:50]}")
                # This should not happen if retry/fallback worked, but log as error
                logger.error("   âš ï¸ Article will be skipped - no cover image available")
                raise ValueError("Cover image is mandatory but missing after all retries and fallback")
            payload["cover_image"] = enriched.cover_image
            logger.info(f"   ğŸ“· Cover image included: {enriched.cover_image}")

        try:
            # Add API key if available
            headers = {"Content-Type": "application/json"}
            api_key = os.getenv("NUZANTARA_API_KEY")
            if api_key:
                headers["X-API-Key"] = api_key
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(endpoint, json=payload, headers=headers)
                response.raise_for_status()
                result = response.json()

                if result.get("duplicate"):
                    logger.debug(
                        f"News Room: Article already in staging - {title[:50]}"
                    )
                    return False
                else:
                    logger.info(
                        f"âœ… Sent to News Room: {result.get('intel_type')} - {title[:50]}"
                    )
                    return True

        except httpx.HTTPError as e:
            logger.warning(f"Failed to send to News Room: {e}")
            return False
        except Exception as e:
            logger.warning(f"News Room send error: {e}")
            return False

    async def process_article(self, article: PipelineArticle) -> PipelineArticle:
        """
        Process a single article through the entire pipeline.

        Steps:
        0. Semantic Deduplication (Qdrant)
        1. LLAMA scoring (fast)
        2. Claude validation (for medium scores)
        3. Claude Max enrichment (if approved)
        4. Image reasoning (if enriched)
        """

        logger.info(f"\n{'=' * 60}")
        logger.info(f"ğŸ“° Processing: {article.title[:50]}...")
        logger.info(f"{'=' * 60}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 0: SEMANTIC DEDUPLICATION (The Gatekeeper)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        logger.info("ğŸ§  Step 0: Semantic Deduplication Check...")
        
        try:
            is_dup, original_title, score = await self.deduplicator.is_duplicate(
                article.title, 
                article.summary, 
                article.url
            )
            
            if is_dup:
                logger.warning(f"   ğŸ›‘ DUPLICATE DETECTED (Score: {score:.2f})")
                logger.warning(f"      Original: {original_title}")
                article.is_duplicate = True
                article.duplicate_of = original_title
                article.similarity_score = score
                self.stats.dedup_filtered += 1
                return article
            
            logger.info("   âœ… New unique content")
            
        except Exception as e:
            logger.error(f"   Dedup check failed (continuing safely): {e}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 1: LLAMA SCORING
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        logger.info("\nğŸ“Š Step 1: LLAMA Scoring...")

        try:
            # Use professional scorer (keyword-based) + Ollama enhancement
            score_result = await score_article(
                title=article.title,
                content=article.summary or article.content[:500],
                source=article.source,
                source_url=article.url,
                use_ollama=True,  # Enhance with Ollama for edge cases
            )

            article.llama_score = score_result.final_score
            article.llama_category = score_result.matched_category
            article.llama_priority = score_result.priority
            article.llama_reason = score_result.explanation
            article.llama_keywords = score_result.matched_keywords

            self.stats.llama_scored += 1

            logger.info(f"   Score: {article.llama_score}/100")
            logger.info(
                "ğŸ“Š LLAMA Scoring Results",
                extra={
                    "score": article.llama_score,
                    "category": article.llama_category,
                    "priority": article.llama_priority,
                    "keywords": article.llama_keywords[:10],
                    "source": article.source,
                },
            )

        except Exception as e:
            logger.error(f"   LLAMA scoring failed: {e}")
            article.llama_score = 50  # Default
            article.llama_category = "general"
            self.stats.errors += 1

        # Filter by minimum score
        if article.llama_score < self.min_llama_score:
            logger.info(
                f"   âŒ Filtered (score {article.llama_score} < {self.min_llama_score})"
            )
            self.stats.llama_filtered += 1
            return article

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 2: CLAUDE VALIDATION
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        logger.info("\nğŸ” Step 2: Claude Validation...")

        try:
            validation = await self.claude_validator.validate_article(
                title=article.title,
                summary=article.summary,
                content=article.content or article.summary,
                source=article.source,
                llama_score=article.llama_score,
                llama_category=article.llama_category,
                llama_reason=article.llama_reason,
            )

            article.validated = True
            article.validation_approved = validation.approved
            article.validation_confidence = validation.confidence
            article.validation_reason = validation.reason
            article.category_override = validation.category_override
            article.priority_override = validation.priority_override
            article.enrichment_hints = validation.enrichment_hints or []

            self.stats.claude_validated += 1

            if validation.approved:
                self.stats.claude_approved += 1
                logger.success(f"   âœ… Approved (confidence: {validation.confidence})")
                logger.info(
                    "ğŸ” Claude Validation Success",
                    extra={
                        "confidence": validation.confidence,
                        "category_override": validation.category_override,
                        "priority_override": validation.priority_override,
                        "reason": validation.reason,
                    },
                )
            else:
                self.stats.claude_rejected += 1
                logger.info(f"   âŒ Rejected (confidence: {validation.confidence})")
                logger.info(f"   Reason: {validation.reason}")
                logger.info(
                    "ğŸ” Claude Validation Rejected",
                    extra={
                        "confidence": validation.confidence,
                        "reason": validation.reason,
                    },
                )
                return article

            if validation.category_override:
                logger.info(
                    f"   Category override: {article.llama_category} â†’ {validation.category_override}"
                )
            if validation.priority_override:
                logger.info(
                    f"   Priority override: {article.llama_priority} â†’ {validation.priority_override}"
                )

        except Exception as e:
            logger.error(f"   Claude validation failed: {e}")
            # On validation error, approve if score is decent
            article.validation_approved = article.llama_score >= 55
            self.stats.errors += 1

        if not article.validation_approved:
            return article

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 3: CLAUDE MAX ENRICHMENT
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        logger.info("\nâœï¸ Step 3: Claude Max Enrichment...")

        if self.dry_run:
            logger.info("   [DRY RUN] Skipping enrichment")
            return article

        try:
            enriched = await self.enricher.enrich_article(
                title=article.title,
                summary=article.summary,
                source_url=article.url,
                source=article.source,
                category=article.final_category,
                published_at=article.published_at,
            )

            if enriched:
                article.enriched = True
                article.enriched_article = enriched
                self.stats.enriched += 1

                logger.success(f"   âœ… Enriched: {enriched.headline[:50]}...")
                logger.info(
                    "âœï¸ Article Enrichment Results",
                    extra={
                        "headline": enriched.headline,
                        "category": enriched.category,
                        "priority": enriched.priority,
                        "relevance_score": enriched.relevance_score,
                        "fact_count": len(enriched.facts.split("\n"))
                        if enriched.facts
                        else 0,
                    },
                )
            else:
                logger.error("   âŒ Enrichment failed")
                self.stats.errors += 1

        except Exception as e:
            logger.error(f"   Enrichment error: {e}")
            self.stats.errors += 1

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 4: IMAGE GENERATION (mandatory)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if article.enriched:
            # Image generation is mandatory for enriched articles
            if not self.image_generator:
                logger.error("   âŒ Image generator not available (mandatory)")
                self.stats.errors += 1
            elif not article.enriched_article.cover_image:
                logger.warning("   âš ï¸ Cover image not generated during enrichment (mandatory)")
                self.stats.errors += 1
            logger.info("\nğŸ¨ Step 4: Image Reasoning Prepared...")
            logger.info("   Image context saved for Claude to reason about")
            logger.info("   Claude will create unique prompt based on article content")
            # Check if image was prepared in enricher
            if article.enriched_article and article.enriched_article.cover_image:
                self.stats.images_generated += 1
                logger.success(f"   âœ… Image prepared: {article.enriched_article.cover_image}")
            # Actual image generation happens in enricher with browser automation

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 5.5: SEO/AEO OPTIMIZATION
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if article.enriched and article.enriched_article:
            logger.info("\nğŸ” Step 5.5: SEO/AEO Optimization...")

            try:
                # Build article dict for SEO optimizer
                enriched = article.enriched_article
                # Combine article sections for SEO content
                full_content = f"{enriched.facts}\n\n{enriched.bali_zero_take}"
                article_data = {
                    "title": enriched.headline,
                    "content": full_content,
                    "enriched_content": full_content,
                    "category": enriched.category,
                    "source_url": article.url,
                    "image_url": article.image_path or "",
                    "published_date": article.published_at
                    or datetime.now().isoformat(),
                }

                # Optimize for SEO/AEO
                optimized = seo_optimize(article_data)
                article.seo_metadata = optimized.get("seo", {})
                article.seo_optimized = True
                self.stats.seo_optimized += 1

                logger.success("   âœ… SEO/AEO optimization complete")
                logger.info(
                    f"   Title: {article.seo_metadata.get('title', '')[:50]}..."
                )
                logger.info(
                    f"   Keywords: {', '.join(article.seo_metadata.get('keywords', [])[:5])}"
                )
                logger.info(
                    f"   Entities: {', '.join(article.seo_metadata.get('key_entities', [])[:5])}"
                )
                logger.info(
                    f"   FAQs: {len(article.seo_metadata.get('faq_items', []))} generated"
                )

            except Exception as e:
                logger.error(f"   âŒ SEO optimization failed: {e}")
                self.stats.errors += 1

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 6: SUBMIT FOR APPROVAL (Preview + Telegram + News Room)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Submit if enriched (SEO is optional, enrichment is required)
        if article.enriched and article.enriched_article:
            logger.info("\nğŸ“¨ Step 6: Submitting for Approval...")

            preview_url = None  # Will be set by approval system

            # 6a. Generate preview and submit to Telegram FIRST (to get preview_url)
            if self.require_approval and self.approval_system:
                try:
                    enriched = article.enriched_article
                    if not enriched:
                        logger.warning("   âš ï¸ Cannot submit: enriched_article is None")
                        return article

                    # Build enriched content from available fields
                    enriched_content = enriched.ai_summary or ""
                    if enriched.facts:
                        enriched_content += "\n\n" + enriched.facts
                    if enriched.bali_zero_take:
                        enriched_content += "\n\n" + enriched.bali_zero_take

                    article_data = {
                        "title": enriched.headline or article.title,
                        "content": enriched_content or article.summary,
                        "enriched_content": enriched_content or article.summary,
                        "category": enriched.category or article.final_category,
                        "source": article.source,
                        "source_url": article.url,
                        "image_url": enriched.cover_image or article.image_path or "",
                        "relevance_score": article.llama_score,
                        "published_at": article.published_at,
                    }

                    pending = await self.approval_system.submit_for_approval(
                        article=article_data,
                        seo_metadata=article.seo_metadata,
                        enriched_content=enriched_content or article.summary,
                    )

                    article.pending_approval = True
                    article.approval_id = pending.article_id
                    article.approval_status = "pending"
                    article.preview_url = pending.preview_url  # Store on article
                    article.requires_human_review = self.require_human_review  # E-E-A-T flag
                    preview_url = pending.preview_url  # Get preview URL for news room
                    self.stats.pending_approval += 1
                    if self.require_human_review:
                        self.stats.pending_human_review += 1

                    logger.success("   âœ… Submitted for approval")
                    logger.info(f"   Article ID: {pending.article_id}")
                    logger.info(f"   ğŸ–¼ï¸ Preview URL: {pending.preview_url}")
                    if self.require_human_review:
                        logger.info("   ğŸ‘¤ E-E-A-T: Human review REQUIRED before publish")
                    if pending.telegram_message_id:
                        logger.info("   ğŸ“± Telegram notification sent!")
                    else:
                        logger.warning(
                            "   âš ï¸ Telegram notification not sent (check config)"
                        )

                except Exception as e:
                    logger.error(f"   âŒ Approval submission failed: {e}")
                    self.stats.errors += 1

            # 6b. Send to News Room WITH preview_url (zantara.balizero.com/intelligence/news-room)
            if not self.dry_run:
                try:
                    news_room_sent = await self.send_to_news_room(article, preview_url=preview_url)
                    if news_room_sent:
                        logger.info("   ğŸ“° Sent to News Room UI (with preview link)")
                except Exception as e:
                    logger.warning(f"   âš ï¸ News Room submission failed: {e}")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 7: AUTO-MEMORY (Save to Qdrant)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Save ALL approved articles to memory, not just pending approval
        # This ensures future deduplication works even if Telegram fails
        if article.validation_approved and article.enriched and not self.dry_run:
            logger.info("\nğŸ’¾ Step 7: Saving to Semantic Memory...")
            try:
                # Usa il titolo/contenuto arricchito per l'embedding (piÃ¹ preciso)
                enriched = article.enriched_article
                if not enriched:
                    logger.warning("   âš ï¸ Cannot save: enriched_article is None")
                    return article
                
                await self.deduplicator.save_article({
                    "title": enriched.headline or article.title,
                    "summary": enriched.ai_summary or article.summary[:500], # Short AI summary
                    "content": enriched.facts or enriched.ai_summary or article.summary,      # Full facts for deeper context
                    "url": article.url,
                    "source": article.source,
                    "category": article.final_category,
                    "publishedAt": article.published_at,
                    "tier": "T2"
                })
                logger.success("   âœ… Saved to Qdrant (Future deduplication enabled)")
                
            except Exception as e:
                logger.error(f"   Memory save failed: {e}")

        return article

    async def process_batch(
        self, articles: List[Dict]
    ) -> Tuple[List[PipelineArticle], PipelineStats]:
        """
        Process a batch of articles through the pipeline.

        Args:
            articles: List of article dicts with keys:
                     title, summary, url, source, content (optional)

        Returns:
            (processed_articles, stats)
        """
        import time

        start_time = time.time()

        # Reset and start metrics
        self.metrics.reset()
        self.metrics.start_pipeline()

        self.stats = PipelineStats()
        self.stats.total_input = len(articles)

        # Track input count in metrics
        self.metrics.increment("articles_input", len(articles))

        self.metrics_logger.info(f"Processing batch", article_count=len(articles))
        logger.info("=" * 70)
        logger.info(f"ğŸš€ PROCESSING BATCH: {len(articles)} articles")
        logger.info("=" * 70)

        results = []

        for i, article_dict in enumerate(articles, 1):
            logger.info(f"\n[{i}/{len(articles)}] {'=' * 50}")

            # Validate URL before creating article
            # Support multiple URL field names: url, source_url, sourceUrl
            url = (
                article_dict.get("url") 
                or article_dict.get("source_url") 
                or article_dict.get("sourceUrl")
                or ""
            )
            if not url or not url.startswith(("http://", "https://")):
                logger.warning(f"Skipping article with invalid URL: {url}")
                self.stats.errors += 1
                continue
            
            # Create pipeline article
            article = PipelineArticle(
                title=article_dict.get("title", ""),
                summary=article_dict.get("summary", ""),
                url=url,
                source=article_dict.get("source", "Unknown"),
                content=article_dict.get("content", ""),
                published_at=article_dict.get("published_at"),
            )

            # Process through pipeline
            processed = await self.process_article(article)
            results.append(processed)

            # Adaptive rate limiting - faster for successful processing
            # Base: 0.5s, increases on errors, maxes at 3s
            if processed.enriched:
                delay = 0.5  # Fast for successful enrichment
            elif processed.validation_approved:
                delay = 0.3  # Very fast for validated but not enriched
            elif processed.is_duplicate or processed.llama_score < self.min_llama_score:
                delay = 0.2  # Near-instant for filtered articles
            else:
                delay = 1.0  # Default for other cases

            await asyncio.sleep(delay)

        self.stats.duration_seconds = time.time() - start_time

        # Print summary
        self._print_summary()

        # Save batch run audit log
        try:
            audit_dir = Path("logs/audit")
            audit_dir.mkdir(exist_ok=True, parents=True)
            audit_file = (
                audit_dir / f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            with open(audit_file, "w") as f:
                json.dump(
                    {
                        "timestamp": datetime.now().isoformat(),
                        "stats": asdict(self.stats),
                        "results_count": len(results),
                    },
                    f,
                    indent=2,
                    default=str, # Fix for serialization issues
                )
            logger.info(f"ğŸ“ Run audit saved to {audit_file}")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to save run audit: {e}")

        # Finalize metrics
        self.metrics.end_pipeline()

        # Save metrics to file alongside audit log
        try:
            metrics_file = audit_dir / f"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            self.metrics.save_to_file(str(metrics_file))
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to save metrics: {e}")

        # Cleanup resources
        if hasattr(self.deduplicator, 'close'):
            self.deduplicator.close()

        return results, self.stats

    def _print_summary(self):
        """Print pipeline execution summary"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š PIPELINE SUMMARY")
        logger.info("=" * 70)
        logger.info(f"   Total input:         {self.stats.total_input}")
        logger.info(f"   Dedup filtered:      {self.stats.dedup_filtered}")
        logger.info(f"   LLAMA scored:        {self.stats.llama_scored}")
        logger.info(f"   LLAMA filtered:      {self.stats.llama_filtered}")
        logger.info(f"   Claude validated:    {self.stats.claude_validated}")
        logger.info(f"   Claude approved:     {self.stats.claude_approved}")
        logger.info(f"   Claude rejected:     {self.stats.claude_rejected}")
        logger.info(f"   Enriched:            {self.stats.enriched}")
        logger.info(f"   Images generated:    {self.stats.images_generated}")
        logger.info(f"   SEO optimized:       {self.stats.seo_optimized}")
        logger.info(f"   Pending approval:    {self.stats.pending_approval}")
        logger.info(f"   ğŸ‘¤ Pending review:   {self.stats.pending_human_review} (E-E-A-T)")
        logger.info(f"   Published:           {self.stats.published}")
        logger.info(f"   Errors:              {self.stats.errors}")
        logger.info(f"   Duration:            {self.stats.duration_seconds:.1f}s")
        logger.info("=" * 70)

    def get_metrics(self) -> dict:
        """Get current metrics as dictionary"""
        return self.metrics.to_dict()

    def get_prometheus_metrics(self) -> str:
        """Get metrics in Prometheus format"""
        return self.metrics.export_prometheus()


async def test_pipeline():
    """Test the complete pipeline"""

    pipeline = IntelPipeline(
        min_llama_score=40,
        auto_approve_threshold=75,
        generate_images=False,  # Disable for test
        require_approval=False,  # Disable for test
        dry_run=True,  # Don't actually enrich
    )

    test_articles = [
        {
            "title": "Indonesia Extends Digital Nomad Visa to 5 Years",
            "summary": "The Indonesian government announced a groundbreaking policy extending the digital nomad visa validity from 1 year to 5 years.",
            "url": "https://example.com/nomad-visa",
            "source": "Jakarta Post",
        },
        {
            "title": "New Coretax System Causing NPWP Registration Problems",
            "summary": "The new Coretax tax system is experiencing errors and delays for NPWP registration.",
            "url": "https://example.com/coretax",
            "source": "Bisnis Indonesia",
        },
        {
            "title": "Bali Cultural Festival Attracts Thousands",
            "summary": "Annual festival celebrates traditional Balinese arts and culture.",
            "url": "https://example.com/festival",
            "source": "Tribun Bali",
        },
    ]

    results, stats = await pipeline.process_batch(test_articles)

    logger.info("=" * 70)
    logger.info("RESULTS")
    logger.info("=" * 70)

    for article in results:
        if article.is_duplicate:
            logger.info(f"DUPLICATE: {article.title[:50]}... Of: {article.duplicate_of}")
            continue

        status = "APPROVED" if article.validation_approved else "REJECTED"
        logger.info(f"{status}: {article.title[:50]}...")
        logger.info(f"   LLAMA: {article.llama_score} ({article.llama_category})")
        logger.info(f"   Validated: {article.validated}, Approved: {article.validation_approved}")
        logger.info(f"   Reason: {article.validation_reason}")
        if article.seo_optimized:
            logger.info("   SEO: Optimized")
            logger.info(f"   Keywords: {', '.join(article.seo_metadata.get('keywords', [])[:5])}")
            logger.info(f"   Entities: {', '.join(article.seo_metadata.get('key_entities', [])[:3])}")


if __name__ == "__main__":
    asyncio.run(test_pipeline())
