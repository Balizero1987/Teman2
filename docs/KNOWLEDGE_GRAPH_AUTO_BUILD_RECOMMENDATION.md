# ‚úÖ KNOWLEDGE GRAPH AUTO-BUILD - Raccomandazione Finale

**Data:** 2026-01-10  
**Domanda:** √à fattibile e best practice rendere il build automatico per popolare il KG?

---

## ‚úÖ RISPOSTA: S√å, Fattibile E Best Practice

**Conclusione:** ‚úÖ **S√¨, √® fattibile E best practice**, con implementazione incrementale intelligente.

---

## üéØ RACCOMANDAZIONE FINALE

### Strategia Consigliata: Incremental Automatico

**Approccio:**
- ‚úÖ **Build incrementale automatico** ogni 6-12 ore
- ‚úÖ **Solo chunk nuovi/modificati** (non full rebuild)
- ‚úÖ **Collezioni prioritarie** (alta priorit√†)
- ‚úÖ **Gemini per estrazione** (pi√π economico)
- ‚úÖ **Error handling robusto**

---

## üí∞ ANALISI COSTI

### Costo Full Build (58k docs)

**Claude Sonnet 4:**
- Costo/chunk: ~$0.0021
- Totale: ~$122 (full build)
- Tempo: ~19 ore

**Gemini (raccomandato):**
- Costo/chunk: ~$0.0005
- Totale: ~$29 (full build)
- Tempo: ~19 ore

### Costo Incremental (solo nuovi)

**Assumendo 10% nuovi chunk/giorno:**
- Chunk nuovi: ~5,800/giorno
- Costo/giorno: ~$2.90 (Gemini)
- Costo/mese: ~$87

**Assumendo 5% nuovi chunk/giorno:**
- Chunk nuovi: ~2,900/giorno
- Costo/giorno: ~$1.45 (Gemini)
- Costo/mese: ~$43.50

**Raccomandazione:** ‚úÖ **Incremental √® 10-20x pi√π economico**

---

## ‚è±Ô∏è ANALISI PERFORMANCE

### Tempi Stimati

**Full Build (58k docs):**
- Batch size: 10, Concurrent: 5
- Tempo: ~19 ore
- Throughput: ~50 chunk/minuto

**Incremental (5,800 nuovi chunk):**
- Tempo: ~2 ore
- Throughput: ~50 chunk/minuto

**Incremental (2,900 nuovi chunk):**
- Tempo: ~1 ora
- Throughput: ~50 chunk/minuto

**Ottimizzazioni:**
- Batch size: 20 ‚Üí Tempo: ~0.5-1 ora
- Concurrent: 10 ‚Üí Tempo: ~0.25-0.5 ore

---

## üõ†Ô∏è IMPLEMENTAZIONE

### Script Esistente ‚úÖ

**File:** `apps/backend-rag/scripts/kg_incremental_extraction.py`

**Funzionalit√†:**
- ‚úÖ Traccia chunk processati (`get_processed_chunk_ids()`)
- ‚úÖ Filtra solo chunk nuovi
- ‚úÖ Processa in batch paralleli
- ‚úÖ Usa Gemini (economico)
- ‚úÖ Error handling con retry

**Metodo Chiave:**
```python
async def get_processed_chunk_ids(self) -> set:
    """Get all chunk IDs already in KG."""
    query = "SELECT DISTINCT unnest(source_chunk_ids) as chunk_id FROM kg_nodes WHERE source_chunk_ids IS NOT NULL"
    rows = await self.db_pool.fetch(query)
    return set(r["chunk_id"] for r in rows if r["chunk_id"])
```

**Utilizzo:**
```python
# Filtra solo chunk non processati
unprocessed = [c for c in all_chunks if c["id"] not in processed_ids]
```

---

### Modifiche Necessarie al Scheduler

**File:** `apps/backend-rag/backend/services/misc/autonomous_scheduler.py`

**Modifica Task:**
```python
async def run_knowledge_graph_builder():
    """Build knowledge graph incrementally"""
    # 1. Schema
    await graph_builder.init_graph_schema()
    
    # 2. Conversazioni (sempre)
    await graph_builder.build_graph_from_all_conversations(days_back=7)
    
    # 3. Qdrant Collections (incremental)
    from scripts.kg_incremental_extraction import KGIncrementalExtractor
    
    extractor = KGIncrementalExtractor(
        db_pool=db_pool,
        qdrant_url=settings.qdrant_url,
        qdrant_api_key=settings.qdrant_api_key,
        gemini_client=gemini_client
    )
    
    # Solo collezioni alta priorit√†
    high_priority = [
        "legal_unified_hybrid",
        "kbli_unified",
        "tax_genius_hybrid"
    ]
    
    await extractor.run(
        collections=high_priority,
        limit=None,  # Tutti i nuovi chunk
        dry_run=False
    )
```

**Frequenza:**
- Ogni 6-12 ore (raccomandato)
- O ogni 4 ore (allineato con altri agent)

---

## üìä BEST PRACTICES IDENTIFICATE

### 1. Incremental Updates ‚úÖ

**Perch√©:**
- ‚úÖ Costo 10-20x inferiore
- ‚úÖ Tempo 10-20x inferiore
- ‚úÖ Aggiornamento continuo
- ‚úÖ Evita duplicati

**Come:**
- Usa `source_chunk_ids` per tracking
- Filtra chunk gi√† processati
- Processa solo nuovi/modificati

---

### 2. Collezioni Prioritarie ‚úÖ

**Alta Priorit√† (ogni 6-12h):**
- `legal_unified_hybrid` (47,959 docs)
- `kbli_unified` (2,818 docs)
- `tax_genius_hybrid` (332 docs)

**Media Priorit√† (ogni 24h):**
- `visa_oracle` (82 docs)
- `bali_zero_pricing` (70 docs)

**Bassa Priorit√† (manuale):**
- `training_conversations_hybrid` (statico)
- `balizero_news_history` (piccolo)

---

### 3. Gemini per Estrazione ‚úÖ

**Perch√©:**
- ‚úÖ 4x pi√π economico di Claude
- ‚úÖ Performance simili per estrazione
- ‚úÖ Rate limit pi√π alti (60+ RPM)

**Configurazione:**
```python
extractor_type: str = "gemini"  # Invece di "claude"
model: str = "gemini-1.5-flash"  # Economico e veloce
```

---

### 4. Error Handling ‚úÖ

**Best Practices:**
- ‚úÖ Retry con exponential backoff
- ‚úÖ Continue on error (non bloccare tutto)
- ‚úÖ Log dettagliato
- ‚úÖ Alert su errori critici

**Implementazione:**
```python
try:
    await extractor.run(collections=high_priority)
except Exception as e:
    logger.error(f"KG extraction failed: {e}")
    # Alert admin ma non bloccare scheduler
```

---

### 5. Monitoring e Alerting ‚úÖ

**Metriche da Tracciare:**
- Chunk processati per run
- Entit√†/relazioni estratte
- Costo LLM per run
- Tempo esecuzione
- Errori e retry

**Alert:**
- Costo giornaliero > threshold
- Errori consecutivi > 3
- Tempo esecuzione > 2 ore

---

## üöÄ PIANO IMPLEMENTAZIONE

### Step 1: Verificare Script Incremental ‚úÖ

**File:** `apps/backend-rag/scripts/kg_incremental_extraction.py`

**Status:** ‚úÖ Gi√† esistente e funzionante

**Verifiche:**
- ‚úÖ Traccia chunk processati
- ‚úÖ Filtra chunk nuovi
- ‚úÖ Usa Gemini (economico)
- ‚úÖ Error handling

---

### Step 2: Modificare Scheduler

**File:** `apps/backend-rag/backend/services/misc/autonomous_scheduler.py`

**Modifiche:**
1. Importare `KGIncrementalExtractor`
2. Modificare `run_knowledge_graph_builder()`
3. Aggiungere estrazione incrementale Qdrant
4. Configurare collezioni prioritarie

**Frequenza:**
- Ogni 6-12 ore (raccomandato)
- O ogni 4 ore (allineato con altri agent)

---

### Step 3: Configurazione

**Variabili Ambiente:**
- `GEMINI_API_KEY` - Per estrazione economica
- `KG_AUTO_BUILD_ENABLED` - Enable/disable
- `KG_COLLECTIONS_PRIORITY` - Collezioni da processare

**Configurazione Pipeline:**
```python
config = PipelineConfig(
    extractor_type="gemini",  # Economico
    batch_size=20,  # Ottimale
    max_concurrent=5,  # Bilanciato
    use_coreference=False,  # Risparmia costo
    min_confidence=0.6
)
```

---

### Step 4: Testing

**Test Incremental:**
1. Eseguire dry-run
2. Verificare chunk filtrati correttamente
3. Testare estrazione su sample
4. Verificare persistenza

**Test Scheduler:**
1. Verificare task eseguito
2. Monitorare costi
3. Verificare errori
4. Testare retry

---

## üìà BENEFICI ATTESI

### Benefici Tecnici

**‚úÖ Knowledge Graph Aggiornato:**
- Entit√†/relazioni sempre aggiornate
- Query pi√π accurate
- Migliore comprensione contesto

**‚úÖ Performance:**
- Query strutturate pi√π veloci
- Context pi√π ricco per AI
- Relazioni validate

### Benefici Business

**‚úÖ Migliori Risposte AI:**
- Context pi√π ricco
- Relazioni strutturate
- Entit√† validate

**‚úÖ Manutenzione Automatica:**
- Nessun intervento manuale
- Aggiornamento continuo
- Sistema self-healing

---

## ‚ö†Ô∏è RISCHI E MITIGAZIONI

### Rischio 1: Costo LLM

**Mitigazione:**
- ‚úÖ Incremental (solo nuovi chunk)
- ‚úÖ Gemini invece di Claude
- ‚úÖ Batch size ottimale
- ‚úÖ Monitoring costi

### Rischio 2: Performance

**Mitigazione:**
- ‚úÖ Processare durante ore non di picco
- ‚úÖ Batch processing parallelo
- ‚úÖ Timeout e retry
- ‚úÖ Background task

### Rischio 3: Errori

**Mitigazione:**
- ‚úÖ Error handling robusto
- ‚úÖ Retry con backoff
- ‚úÖ Continue on error
- ‚úÖ Alert e monitoring

---

## ‚úÖ CONCLUSIONE

### √à Fattibile? ‚úÖ S√å

**Motivi:**
- ‚úÖ Componenti gi√† disponibili
- ‚úÖ Script incremental esistente
- ‚úÖ Scheduler gi√† configurato
- ‚úÖ Tracking chunk implementato

### √à Best Practice? ‚úÖ S√å

**Motivi:**
- ‚úÖ Incremental updates standard
- ‚úÖ Aggiornamento continuo
- ‚úÖ Costi controllati
- ‚úÖ Performance ottimale

### Raccomandazione Finale

**‚úÖ IMPLEMENTARE build automatico incrementale:**

1. **Frequenza:** Ogni 6-12 ore
2. **Strategia:** Incremental (solo nuovi chunk)
3. **Collezioni:** Alta priorit√† solo
4. **Modello:** Gemini (economico)
5. **Costo stimato:** ~$1.5-3/giorno (~$45-90/mese)

**Benefici:**
- ‚úÖ Knowledge Graph sempre aggiornato
- ‚úÖ Costi controllati
- ‚úÖ Performance ottimale
- ‚úÖ Manutenzione automatica

---

**Documentazione creata:** 2026-01-10  
**Raccomandazione:** ‚úÖ **IMPLEMENTARE**
