# Race Conditions & Locking Behavior

**Last Updated:** 2025-12-28  
**Status:** âœ… Implemented

## Overview

This document describes the locking mechanisms implemented to prevent race conditions in the NUZANTARA platform. All locks are designed to maintain data consistency while minimizing performance overhead.

---

## Locking Strategy

### Design Principles

1. **Per-Resource Locks**: Locks are scoped to specific resources (user_id, collection_name, session_id) to maximize concurrency
2. **Read-Write Separation**: Read operations can proceed concurrently, write operations are exclusive
3. **Timeout Protection**: All locks have configurable timeouts to prevent deadlocks
4. **Graceful Degradation**: Lock timeouts don't crash the system, they return empty results or log warnings
5. **Performance First**: Lock overhead is kept under 10ms per operation

---

## Component-Specific Locking

### 1. AgenticRAGOrchestrator - Memory Save

**Location:** `services/rag/agentic/orchestrator.py`

**Lock Type:** `asyncio.Lock` per `user_id`

**Behavior:**
- One lock per user to prevent concurrent memory saves for the same user
- Timeout: 5 seconds
- If timeout occurs: Logs warning, records metric, continues without saving

**Usage:**
```python
async def _save_conversation_memory(self, user_id: str, query: str, answer: str):
    lock = self._memory_locks[user_id]
    try:
        await asyncio.wait_for(lock.acquire(), timeout=5.0)
        try:
            # Save memory...
        finally:
            lock.release()
    except asyncio.TimeoutError:
        # Handle timeout gracefully
```

**Metrics:**
- `zantara_memory_lock_timeout_total{user_id}`: Count of timeouts per user
- `zantara_memory_lock_contention_seconds{operation}`: Time spent waiting for locks

---

### 2. MemoryOrchestrator - Read-Write Locks

**Location:** `services/memory/orchestrator.py`

**Lock Types:**
- **Read Semaphore**: `asyncio.Semaphore(10)` per `user_email` - allows 10 concurrent reads
- **Write Lock**: `asyncio.Lock` per `user_email` - exclusive write access

**Behavior:**

#### Read Operations (`get_user_context`)
- Multiple reads can proceed concurrently (up to 10 per user)
- Reads are not blocked by other reads
- Reads wait for write operations to complete

#### Write Operations (`process_conversation`)
- Exclusive access during fact extraction and saving
- Blocks all reads and writes for the same user
- Timeout: 5 seconds
- If timeout occurs: Returns empty `MemoryProcessResult`

**Usage:**
```python
# Read (concurrent)
async def get_user_context(self, user_email: str):
    semaphore = self._read_semaphores[user_email]
    async with semaphore:
        # Read operations...

# Write (exclusive)
async def process_conversation(self, user_email: str, ...):
    lock = self._write_locks[user_email]
    try:
        await asyncio.wait_for(lock.acquire(), timeout=5.0)
        try:
            # Write operations...
        finally:
            lock.release()
    except asyncio.TimeoutError:
        # Return empty result
```

**Metrics:**
- Same as AgenticRAGOrchestrator

---

### 3. CollectionManager - Collection Access

**Location:** `services/ingestion/collection_manager.py`

**Lock Types:**
- **Read Semaphore**: `asyncio.Semaphore(20)` per `collection_name` - allows 20 concurrent searches
- **Write Lock**: `asyncio.Lock` per `collection_name` - exclusive ingestion access

**Behavior:**

#### Search Operations (`search_with_lock`)
- Multiple searches can proceed concurrently (up to 20 per collection)
- Searches are not blocked by other searches
- Searches wait for ingestion operations to complete

#### Ingestion Operations (`ingest_with_lock`)
- Exclusive access during document ingestion
- Blocks all searches and ingests for the same collection
- Timeout: 30 seconds (longer for ingestion operations)
- If timeout occurs: Raises `RuntimeError`

**Usage:**
```python
# Search (concurrent)
results = await collection_manager.search_with_lock(
    collection_name="visa_oracle",
    query_embedding=embedding,
    limit=5
)

# Ingest (exclusive)
await collection_manager.ingest_with_lock(
    collection_name="visa_oracle",
    documents=docs,
    embeddings=embeddings
)
```

**Metrics:**
- `zantara_collection_lock_timeout_total{collection_name}`: Count of timeouts per collection

**Note:** These methods are available but not yet used everywhere. Migration to locked methods is recommended for production workloads.

---

### 4. ConversationSave - Cache-DB Consistency

**Location:** `app/routers/conversations.py`

**Lock Type:** Two-phase commit pattern (no explicit locks, uses DB transactions)

**Behavior:**
1. **Phase 1**: Save to memory cache (fast, always succeeds)
2. **Phase 2**: Save to PostgreSQL with retry mechanism
   - Up to 3 retry attempts
   - Exponential backoff (2^attempt seconds)
   - Atomic transaction ensures consistency

**Usage:**
```python
# Phase 1: Cache (fast)
mem_cache.add_message(session_id, role, content)

# Phase 2: DB (with retry)
for attempt in range(max_retries):
    try:
        async with conn.transaction():
            # Atomic insert...
            break  # Success
    except Exception:
        if attempt == max_retries - 1:
            # Queue for async retry
        else:
            await asyncio.sleep(2 ** attempt)
```

**Metrics:**
- `zantara_cache_db_consistency_errors_total{session_id}`: Count of consistency errors

---

### 5. CollectiveMemoryService - Fact Promotion

**Location:** `services/memory/collective_memory_service.py`

**Lock Type:** Database-level lock (`SELECT FOR UPDATE`)

**Behavior:**
- Uses PostgreSQL row-level locking to prevent concurrent promotion
- Atomic increment of `source_count` with promotion check
- Idempotent promotion (only happens once when threshold crossed)

**Usage:**
```python
async with conn.transaction():
    # Lock row
    fact = await conn.fetchrow(
        "SELECT ... FROM collective_memories WHERE content_hash = $1 FOR UPDATE",
        content_hash
    )
    
    # Atomic operations within transaction
    await conn.execute("INSERT INTO sources ...")
    await conn.execute("UPDATE collective_memories SET source_count = source_count + 1 ...")
```

**Benefits:**
- Database-level locking ensures consistency across distributed systems
- No application-level locks needed
- Automatic deadlock detection by PostgreSQL

---

## Monitoring & Alerting

### Metrics

All lock-related metrics are exposed via Prometheus:

| Metric | Type | Description |
|--------|------|-------------|
| `zantara_memory_lock_timeout_total` | Counter | Memory lock timeouts per user |
| `zantara_memory_lock_contention_seconds` | Histogram | Time spent waiting for memory locks |
| `zantara_collection_lock_timeout_total` | Counter | Collection lock timeouts per collection |
| `zantara_cache_db_consistency_errors_total` | Counter | Cache-DB consistency errors per session |

### Alerting Rules

Prometheus alerts are configured in `config/prometheus/alerts.yml`:

- **HighMemoryLockContention**: Average lock wait time > 0.1s for 5 minutes
- **MemoryLockTimeouts**: Any lock timeout detected for 2 minutes
- **CriticalMemoryLockTimeouts**: Lock timeout rate > 0.1/s for 1 minute
- **CollectionLockTimeouts**: Collection lock timeout detected for 2 minutes
- **CriticalCollectionLockTimeouts**: Collection lock timeout rate > 0.05/s for 1 minute
- **CacheDBConsistencyErrors**: Consistency errors detected for 5 minutes
- **CriticalCacheDBConsistencyErrors**: Consistency error rate > 0.1/s for 2 minutes

---

## Performance Characteristics

### Lock Overhead

- **Memory locks**: < 1ms per operation (when not contended)
- **Collection locks**: < 1ms per operation (when not contended)
- **Database locks**: Managed by PostgreSQL (negligible overhead)

### Contention Scenarios

1. **High concurrent writes for same user**: Writes are serialized, reads can proceed
2. **High concurrent reads**: Up to 10-20 concurrent reads per resource
3. **Ingestion during search**: Searches wait for ingestion to complete

### Timeout Behavior

- **Memory operations**: Timeout after 5 seconds, continue without saving
- **Collection operations**: Timeout after 30 seconds, raise error
- **Database operations**: Managed by PostgreSQL connection timeout

---

## Best Practices

### For Developers

1. **Use locked methods**: Prefer `search_with_lock()` and `ingest_with_lock()` when available
2. **Handle timeouts**: Always handle `asyncio.TimeoutError` gracefully
3. **Monitor metrics**: Check lock contention metrics in Grafana
4. **Avoid long operations**: Keep critical sections short to minimize lock hold time

### For Operations

1. **Monitor alerts**: Set up alerting for lock timeouts and contention
2. **Investigate timeouts**: High timeout rates indicate system overload
3. **Scale horizontally**: Lock contention is per-resource, scaling helps
4. **Review timeout values**: Adjust timeouts based on observed latencies

---

## Troubleshooting

### High Lock Contention

**Symptoms:**
- High `memory_lock_contention_seconds` values
- Frequent lock timeouts

**Solutions:**
- Check for long-running operations holding locks
- Consider increasing semaphore limits for reads
- Review timeout values
- Scale horizontally

### Lock Timeouts

**Symptoms:**
- `memory_lock_timeout_total` increasing
- Operations failing silently

**Solutions:**
- Check system load
- Review lock timeout values (may need increase)
- Investigate deadlock scenarios
- Check for stuck operations

### Cache-DB Consistency Errors

**Symptoms:**
- `cache_db_consistency_errors_total` increasing
- Conversations not persisting

**Solutions:**
- Check database connectivity
- Review retry mechanism logs
- Verify cache and DB are in sync
- Check for network issues

---

## Future Improvements

1. **Redis-based distributed locks**: For multi-instance deployments
2. **Lock metrics dashboard**: Grafana dashboard for lock monitoring
3. **Automatic timeout adjustment**: Dynamic timeout based on system load
4. **Lock profiling**: Detailed profiling of lock contention patterns

---

## References

- [Race Conditions Fix Prompt](FIX_PROMPTS/RACE_CONDITIONS_FIX.md)
- [Prometheus Alerts](config/prometheus/alerts.yml)
- [Test Suite](tests/unit/services/)

---

*Documentation generated: 2025-12-28*








